# -*- coding: utf-8 -*-
"""regenerate_review_using_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KrbEeFHF_OjMTdvzgg9e4coHtLuC7k19
"""

import torch
import pandas as pd
from datasets import load_dataset
import os
import requests
from PIL import Image
from io import BytesIO
import argparse
import logging
import os
import random
import sys
#from datasetComplaint import LLaVAComplaintDataset
from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, AutoTokenizer, LlavaOnevisionForConditionalGeneration
import numpy as np
import torch
from datasets import load_from_disk
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from transformers.trainer_utils import get_last_checkpoint
from sklearn.model_selection import train_test_split
#from evaluate import load as load_metric
from peft import LoraConfig, get_peft_model, TaskType
from transformers import DefaultDataCollator
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os
from huggingface_hub import InferenceClient
#from dotenv import load_dotenv
from tqdm import tqdm
from transformers import TrainerCallback
#load_dotenv()


os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.cuda.empty_cache()
device = "cuda:0" if torch.cuda.is_available() else "cpu"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,  # Use float16 instead of bfloat16
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,  # Enable double quantization
)

# Load model and tokenizer ONCE at the top
model_id = "meta-llama/Llama-3.2-3B-Instruct"
hf_token = os.environ('HF_TOKEN_personal')
tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(model_id, token=hf_token, quantization_config=quantization_config, torch_dtype="auto", device_map="auto").to(device)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto", token=hf_token)
torch.cuda.empty_cache()

class EmptyCacheCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        torch.cuda.empty_cache()
        print(f"Cleared CUDA cache after epoch {state.epoch}")

def improve_review(review):
    messages = [
        {
            "role": "system",
            "content": (
                "You are an expert review rewriter. "
                "First, determine whether the review is positive or negative. "
                "Then, rewrite the review to be much longer, more detailed, clearer, and more informative, elaborating on the detected sentiment. "
                "If the review is positive, expand on the positive aspects and provide more helpful context. "
                "If the review is negative, elaborate on the complaints and issues. "
                "Make the rewritten review as comprehensive as possible. Answer in plain text. Just the review and do not add any heading."
            ),
        },
        {
            "role": "user",
            "content": f"Original review: {review}",
        }
    ]
    result = pipe(messages, max_new_tokens=1024)

    print(result[0]['generated_text'][-1]['content'].split("**Rewritten Review:**")[-1].strip())
    return result[0]['generated_text'][-1]['content'].split("**Rewritten Review:**")[-1].strip()
#

# Apply to your DataFrame (this will be slow and cost money for large datasets!)


# Use GPU if available

# Save to CSV
#df.to_csv("complaints_with_images.csv", index=False, encoding="utf-8")
#df = pd.DataFrame(data_rows)
df = pd.read_csv("complaints_with_images.csv")[:3000]  # Reduce to 5 samples for testing
#df = df.dropna(subset=['image_url', 'review_body', 'annotation'])
#label_map = {"Complaint": 1, "Non-Complaint": 0}
#df["label"] = df["annotation"].map(label_map)
def clean_text(text):
    return str(text).strip().lower()

#df["text"] = df["review_body"] + df["review_body"].apply(lambda x: f"Original review: {x}\n\n"
#        "First, determine whether the review is positive or negative. "
#        "Then, rewrite the review to be much longer, more entailed, clearer, and more informative, elaborating on the detected sentiment. "
#        "If the review is positive, expand on the positive aspects and provide more helpful context. "
#        "If the review is negative, elaborate on the complaints and issues. "
#        "Make the rewritten review as comprehensive as possible.")

tqdm.pandas()
df["regenerated_review"] = df["review_body"].progress_apply(improve_review)

print("Saving to CSV...")
# Create a fresh DataFrame with all original columns plus regenerated review
clean_df = df[['review_id', 'image_url', 'annotation', 'review_body', 'regenerated_review']].copy()

# Ensure all columns are properly formatted
clean_df['review_id'] = clean_df['review_id'].astype(str)
clean_df['image_url'] = clean_df['image_url'].astype(str)
clean_df['annotation'] = clean_df['annotation'].astype(str)
clean_df['review_body'] = clean_df['review_body'].astype(str)
clean_df['regenerated_review'] = clean_df['regenerated_review'].astype(str)

# Save to a fresh CSV file
clean_df.to_csv("complaints_with_regenerated_reviews.csv", index=False, encoding="utf-8")
print("Fresh CSV saved successfully as 'complaints_with_regenerated_reviews.csv'!")