# -*- coding: utf-8 -*-
"""test_llm_complaint_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dX9ev7OuZzAYoZkIgJRgTaMHP6t2aMph
"""

#!pip install bitsandbytes --prefer-binary

#!pip install accelerate

import torch
import pandas as pd
from datasets import load_dataset
import os
import requests
from PIL import Image
from io import BytesIO
import argparse
import logging
import os
import random
import sys
from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, AutoTokenizer, LlavaOnevisionForConditionalGeneration
import numpy as np
import torch
from datasets import load_from_disk
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from transformers.trainer_utils import get_last_checkpoint
from sklearn.model_selection import train_test_split
#from evaluate import load as load_metric
from peft import LoraConfig, get_peft_model, TaskType
from transformers import DefaultDataCollator
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
load_dotenv()
device = "cuda:0" if torch.cuda.is_available() else "cpu"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4"
)

# Load model and tokenizer ONCE at the top
model_id = "meta-llama/Llama-3.2-3B-Instruct"
hf_token = os.environ['HF_TOKEN_personal']
tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(model_id, token=hf_token, quantization_config=quantization_config, torch_dtype="auto", device_map="auto").to(device)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto", token=hf_token)
torch.cuda.empty_cache()
review = "The purple one bled even after I put it in oxy for several hours. What you see here is the laundry bag I had it in. I spilt water in the laundry bin and this is the result. Iâ€™m just glad the only things it bled onto were those sleeves.<br />Also, these are made for someone with a big head."
messages = [
    {
        "role": "system",
        "content": (
            "You are an expert review detector. "
            "determine whether the review is positive or negative. "
            "If it is positive, then it is a 'Complaint', otherwise, it is a 'Non-Complaint'. "
            "Finally, tell, if it is a 'Complaint' or 'Non-Complaint'. "
        ),
    },
    {
        "role": "user",
        "content": f"Original review: {review}",
    }
]
result = pipe(messages, max_new_tokens=1024)
print(result[0]["generated_text"][-1])

review2 = "\n Yes Attitude is everything! What an amazing read! I loved the book and especially the way in which the examples are thrown right at your face. Your attitude will definitely change after reading this book. The author is an efficient motivational speaker and the book shows that through and through. Apt examples with perfect quotes as you read along, it truly is a treat. It's a book that is neither long enough to bore you to death, nor is it too short that the essentials are left out.\n"
review3 = "\n After using it for 2 times, the paint started peeling off. Shame the manufacturer didn't get it painted with a nice heat resistant paint. Overall size is pretty small though it appears much bigger in the product listing. Build quality otherwise is somewhat average. Not VFM at all. Worth Rs.1000-1200max.\n"
review4 = "\n Got the product damaged but rather than returning I fixed at home, the problem was that fan was stopping after sometime and make a weird sound like it is stuck! I found the motor screws were not properly fitted but anyway go for it at your own luck!!\n"
review5 = "\n After washing the purple one, I noticed that it had a big head, which is why it bled. I also noticed that the black one had a small head, which is why it didn't bleed. I'm glad that the only thing it bled onto were the sleeves."

messages = [
    {
        "role": "system",
        "content": (
            "You are an expert complaint detector. "
            "determine the intent, whether it is positive or negative. "
            "If it is negative, then it is a 'Complaint', otherwise, it is a 'Non-Complaint'. "
            "Finally, tell, if it is a 'Complaint' or 'Non-Complaint'. "
        ),
    },
    {
        "role": "user",
        "content": f"Original review: {review5}",
    }
]
result = pipe(messages, max_new_tokens=1024)
print(result[0]["generated_text"][-1])